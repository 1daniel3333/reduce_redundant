{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your current path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import global_param as param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected file name is ['mock1.txt', 'mock2.txt', 'mock3.txt']\n"
     ]
    }
   ],
   "source": [
    "# Get the list of all files\n",
    "file_list = [f for f in os.listdir(param.directory) if os.path.isfile(os.path.join(param.directory, f))]\n",
    "print(f'Selected file name is {file_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decode_raw_file(file_list, col_header:str=param.col_header)->pd.DataFrame:\n",
    "    merging_list = []\n",
    "    for file_name in file_list:\n",
    "        file_location = os.path.join(param.directory, file_name)\n",
    "        # Open the file in read mode\n",
    "        find_header = False\n",
    "        content = [] #format [[1,2,LOT.X3],[1,2,LOT.X2],[1,2,LOT.X1]]\n",
    "        header = [] #format [DieX, DieY, Wafer,...]\n",
    "\n",
    "        with open(file_location, 'r') as file:\n",
    "            # Read the file line by line\n",
    "            for line in file:\n",
    "                if find_header:\n",
    "                    content.append(line.strip().split())\n",
    "                else:\n",
    "                    if col_header in line.strip():\n",
    "                        find_header = True\n",
    "                        header = line.strip().split()\n",
    "            if len(header)!=len(content[0]):\n",
    "                raise ValueError('Length not match!')\n",
    "        df = pd.DataFrame(content, columns=header)\n",
    "        merging_list.append(df)\n",
    "    merged_df = pd.concat(merging_list, ignore_index=True)\n",
    "    return merged_df\n",
    "\n",
    "column_to_keep = ['LOT','waferID','DieX','DieY']\n",
    "\n",
    "def group_data(df:pd.DataFrame, fail_map:dict)->pd.DataFrame:\n",
    "    print(f'You dataframe column had {list(df.columns)}')\n",
    "    for new_col, cols_to_sum in fail_map.items():\n",
    "        for single_col in cols_to_sum:\n",
    "            if single_col not in list(df.columns):\n",
    "                raise ValueError(f'Column name {single_col} not in column name')\n",
    "        df[new_col] = df[cols_to_sum].astype(int).sum(axis=1)\n",
    "\n",
    "    #kepp wanted column\n",
    "    fin_col_to_keep = list(set(list(fail_map.keys()) + list(param.column_to_keep)))\n",
    "    return df[fin_col_to_keep]\n",
    "\n",
    "def pivot_wide_table_to_long(df:pd.DataFrame, fail_map:dict)->pd.DataFrame:\n",
    "    # Group by DieY, waferID, LOT, DieX\n",
    "    grouped_df = df.groupby(param.column_to_keep).first().reset_index()\n",
    "\n",
    "    # Pivot TYPE1 and TYPE2 from wide table to long table\n",
    "    long_df = pd.melt(grouped_df, id_vars=param.column_to_keep, value_vars=list(fail_map.keys()), var_name='FAIL_TYPE', value_name='VALUE')\n",
    "    return long_df\n",
    "\n",
    "def backfill_dummy(df:pd.DataFrame, fail_map:dict)->pd.DataFrame:\n",
    "    # Define the range for DieX and DieY\n",
    "    die_X_range = range(-5, 6)\n",
    "    die_Y_range = range(-4, 5)\n",
    "    \n",
    "    #define columns to create dummy\n",
    "    columns_to_group = ['DieX', 'DieY','waferID','FAIL_TYPE']\n",
    "    column_to_keep = columns_to_group + ['VALUE']\n",
    "    \n",
    "    #change to int format\n",
    "    df['DieX'] = df['DieX'].astype(int)\n",
    "    df['DieY'] = df['DieY'].astype(int)\n",
    "    \n",
    "    #clean unwanted column\n",
    "    df = df[column_to_keep]\n",
    "    \n",
    "    # Create a MultiIndex for all combinations of DieX and DieY\n",
    "    index = pd.MultiIndex.from_product([die_X_range, die_Y_range,list(df['waferID'].unique()),list(fail_map.keys())], names=columns_to_group)\n",
    "\n",
    "    df = df.drop_duplicates(subset=columns_to_group)\n",
    "    df = df.set_index(columns_to_group).reindex(index).reset_index()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def gen_1st_level_to_result_checking(df:pd.DataFrame):\n",
    "    # Pivot the DataFrame\n",
    "    pivot_df = df.pivot_table(index=['waferID', 'DieY', 'FAIL_TYPE'], columns='DieX', values='VALUE').reset_index()\n",
    "\n",
    "    pivot_df.columns.name = None\n",
    "    return pivot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You dataframe column had ['LOT', 'waferID', 'DieX', 'DieY', 'FailBit', 'SB', 'HTB', 'VTB', 'BL', 'partialBL', 'SBL', 'BLK', 'cross', 'others']\n"
     ]
    }
   ],
   "source": [
    "#fail mode is in a wide table format\n",
    "first_level_df = get_decode_raw_file(file_list)\n",
    "first_level_df.to_csv('first_level_df.csv', index=False)\n",
    "\n",
    "second_level_data = group_data(first_level_df, param.fail_type_map)\n",
    "second_level_data.to_csv('second_level_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DieX</th>\n",
       "      <th>DieY</th>\n",
       "      <th>waferID</th>\n",
       "      <th>FAIL_TYPE</th>\n",
       "      <th>VALUE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-5</td>\n",
       "      <td>-4</td>\n",
       "      <td>TKA.01</td>\n",
       "      <td>TYPE1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5</td>\n",
       "      <td>-4</td>\n",
       "      <td>TKA.01</td>\n",
       "      <td>TYPE2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5</td>\n",
       "      <td>-4</td>\n",
       "      <td>TKB</td>\n",
       "      <td>TYPE1</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5</td>\n",
       "      <td>-4</td>\n",
       "      <td>TKB</td>\n",
       "      <td>TYPE2</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5</td>\n",
       "      <td>-4</td>\n",
       "      <td>TKB.02</td>\n",
       "      <td>TYPE1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DieX  DieY waferID FAIL_TYPE    VALUE\n",
       "0    -5    -4  TKA.01     TYPE1      NaN\n",
       "1    -5    -4  TKA.01     TYPE2      NaN\n",
       "2    -5    -4     TKB     TYPE1  35000.0\n",
       "3    -5    -4     TKB     TYPE2     60.0\n",
       "4    -5    -4  TKB.02     TYPE1      NaN"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_table_raw = pivot_wide_table_to_long(second_level_data, param.fail_type_map)\n",
    "long_table = backfill_dummy(long_table_raw, param.fail_type_map)\n",
    "long_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_check_1st_raw = gen_1st_level_to_result_checking(long_table)\n",
    "result_check_1st_raw = result_check_1st_raw.sort_values(by=['FAIL_TYPE', 'DieY'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set DieY and FAIL_TYPE as index\n",
    "df = result_check_1st_raw\n",
    "df.set_index(['FAIL_TYPE','DieY'], inplace=True)\n",
    "\n",
    "# Group by waferID and save to CSV\n",
    "grouped_df = df.groupby('waferID')\n",
    "\n",
    "# Create a new DataFrame to store the combined result\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each group and concatenate them into the combined DataFrame\n",
    "for name, group in grouped_df:\n",
    "    group = group.drop(columns='waferID')\n",
    "    group.columns = pd.MultiIndex.from_product([[name], group.columns])\n",
    "    \n",
    "    # Create a DataFrame with the same shape as group but filled with None\n",
    "    none_df = pd.DataFrame(None, index=group.index, columns=group.columns)\n",
    "    \n",
    "    # Combine combined_df and group, preserving None values\n",
    "    combined_df = pd.concat([combined_df, none_df], axis=1).combine_first(group)\n",
    "\n",
    "combined_df = combined_df.sort_values(by=['FAIL_TYPE', 'DieY'], ascending=[True, False])\n",
    "combined_df = combined_df.reset_index()\n",
    "\n",
    "# Rename columns\n",
    "combined_df = combined_df.rename(columns={'DieY': 'Fail_%',})\n",
    "\n",
    "combined_df.to_csv('grouped.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('grouped.csv')\n",
    "\n",
    "# Copy the first row\n",
    "first_row = df.iloc[0]\n",
    "\n",
    "# Find the indices where FAIL_TYPE changes\n",
    "change_indices = df[df['FAIL_TYPE'].ne(df['FAIL_TYPE'].shift())].index\n",
    "\n",
    "# Create a list to store the new DataFrame rows\n",
    "new_rows = []\n",
    "\n",
    "# Iterate over the DataFrame and insert the copied row at the change points\n",
    "for i in range(len(df)):\n",
    "    if i in change_indices and i != 0:\n",
    "        new_rows.append(first_row)\n",
    "    new_rows.append(df.iloc[i])\n",
    "\n",
    "# Create a new DataFrame from the new rows\n",
    "new_df = pd.DataFrame(new_rows).reset_index(drop=True)\n",
    "remove_index_0= new_df.drop(index=0).reset_index(drop=True)\n",
    "\n",
    "remove_index_0.to_csv('grouped.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange columns\n",
    "# existing_cols = pivot_df.columns.tolist()\n",
    "# cols_to_rearrange = list(map(str, range(-10, 11)))\n",
    "# cols = ['LOT', 'waferID', 'DieY', 'FAIL_TYPE'] + [col for col in cols_to_rearrange if col in existing_cols]\n",
    "# pivot_df = pivot_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df.to_csv('raw_check.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the range for DieX and DieY\n",
    "die_range = range(-10, 11)  # From -10 to 10 including 0\n",
    "\n",
    "# Create a list of dictionaries with the mock data\n",
    "data = []\n",
    "for die_x in die_range:\n",
    "    for die_y in die_range:\n",
    "        data.append({\n",
    "            'LOT': 'TKB',\n",
    "            'waferID': 'TKB',\n",
    "            'DieX': die_x,\n",
    "            'DieY': die_y,\n",
    "            'FailBit': 1,\n",
    "            'SB': 30000,\n",
    "            'HTB': 5000,\n",
    "            'VTB': 30 if die_x != 3 else 300,  # Example variation\n",
    "            'BL': 30,\n",
    "            'partialBL': np.random.choice([0, 20]),  # Randomly choose between 0 and 20\n",
    "            'SBL': np.random.choice([1, 23]),       # Randomly choose between 1 and 23\n",
    "            'BLK': np.random.choice([4, 32]),       # Randomly choose between 4 and 32\n",
    "            'cross': np.random.choice([2, 12]),     # Randomly choose between 2 and 12\n",
    "            'others': 32\n",
    "        })\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.to_csv('mock3.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
