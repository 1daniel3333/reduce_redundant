{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your current path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import global_param as param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected file name is ['mock1.txt', 'mock2.txt', 'mock3.txt']\n"
     ]
    }
   ],
   "source": [
    "# Get the list of all files\n",
    "file_list = [f for f in os.listdir(param.directory) if os.path.isfile(os.path.join(param.directory, f))]\n",
    "print(f'Selected file name is {file_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decode_raw_file(file_list, col_header:str=param.col_header)->pd.DataFrame:\n",
    "    merging_list = []\n",
    "    for file_name in file_list:\n",
    "        file_location = os.path.join(param.directory, file_name)\n",
    "        # Open the file in read mode\n",
    "        find_header = False\n",
    "        content = [] #format [[1,2,LOT.X3],[1,2,LOT.X2],[1,2,LOT.X1]]\n",
    "        header = [] #format [DieX, DieY, Wafer,...]\n",
    "\n",
    "        with open(file_location, 'r') as file:\n",
    "            # Read the file line by line\n",
    "            for line in file:\n",
    "                if find_header:\n",
    "                    content.append(line.strip().split())\n",
    "                else:\n",
    "                    if col_header in line.strip():\n",
    "                        find_header = True\n",
    "                        header = line.strip().split()\n",
    "            if len(header)!=len(content[0]):\n",
    "                raise ValueError('Length not match!')\n",
    "        df = pd.DataFrame(content, columns=header)\n",
    "        merging_list.append(df)\n",
    "    merged_df = pd.concat(merging_list, ignore_index=True)\n",
    "    return merged_df\n",
    "\n",
    "def group_data(df:pd.DataFrame, fail_map:dict)->pd.DataFrame:\n",
    "    print(f'You dataframe column had {list(df.columns)}')\n",
    "    for new_col, cols_to_sum in fail_map.items():\n",
    "        for single_col in cols_to_sum:\n",
    "            if single_col not in list(df.columns):\n",
    "                raise ValueError(f'Column name {single_col} not in column name')\n",
    "        df[new_col] = df[cols_to_sum].astype(int).sum(axis=1)\n",
    "\n",
    "    #kepp wanted column\n",
    "    fin_col_to_keep = list(set(list(fail_map.keys()) + list(param.column_to_keep)))\n",
    "    return df[fin_col_to_keep]\n",
    "\n",
    "def pivot_wide_table_to_long(df:pd.DataFrame, fail_map:dict)->pd.DataFrame:\n",
    "    # Group by DieY, waferID, LOT, DieX\n",
    "    grouped_df = df.groupby(param.column_to_keep).first().reset_index()\n",
    "\n",
    "    # Pivot TYPE1 and TYPE2 from wide table to long table\n",
    "    long_df = pd.melt(grouped_df, id_vars=param.column_to_keep, value_vars=list(fail_map.keys()), var_name='FAIL_TYPE', value_name='VALUE')\n",
    "    return long_df\n",
    "\n",
    "def backfill_dummy(df:pd.DataFrame, fail_map:dict)->pd.DataFrame:\n",
    "    #define columns to create dummy\n",
    "    columns_to_group = param.column_to_keep+ ['FAIL_TYPE']\n",
    "    \n",
    "    #change to int format\n",
    "    for name in [param.die_x_name,param.die_y_name]:\n",
    "        df[name] = df[name].astype(int)\n",
    "        \n",
    "    #clean unwanted column\n",
    "    column_to_keep_local = columns_to_group + ['VALUE']\n",
    "    df = df[column_to_keep_local]\n",
    "    \n",
    "    # Create a MultiIndex for all combinations of DieX and DieY\n",
    "    index = pd.MultiIndex.from_product([param.die_X_range, param.die_Y_range,list(df[param.wafer_name].unique()),list(fail_map.keys())], names=columns_to_group)\n",
    "\n",
    "    df = df.drop_duplicates(subset=columns_to_group)\n",
    "    df = df.set_index(columns_to_group).reindex(index).reset_index()\n",
    "    return df\n",
    "\n",
    "def gen_1st_level_to_result_checking(df:pd.DataFrame):\n",
    "    # Pivot the DataFrame\n",
    "    pivot_df = df.pivot_table(index=[param.wafer_name, param.die_y_name, 'FAIL_TYPE'], columns=[param.die_x_name], values='VALUE').reset_index()\n",
    "    pivot_df.columns.name = None\n",
    "    # pivot_df = pivot_df.sort_values(by=['FAIL_TYPE', param.die_y_name], ascending=[True, False])\n",
    "    return pivot_df\n",
    "\n",
    "def gen_pivot_to_wide_form_by_wafer(df:pd.DataFrame)->pd.DataFrame:\n",
    "    # Set DieY and FAIL_TYPE as index\n",
    "    df.set_index(['FAIL_TYPE',param.die_y_name], inplace=True)\n",
    "\n",
    "    # Group by waferID and save to CSV\n",
    "    grouped_df = df.groupby(param.wafer_name)\n",
    "\n",
    "    # Create a new DataFrame to store the combined result\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each group and concatenate them into the combined DataFrame\n",
    "    for name, group in grouped_df:\n",
    "        group = group.drop(columns=param.wafer_name)\n",
    "        group.columns = pd.MultiIndex.from_product([[name], group.columns])\n",
    "        \n",
    "        # Create a DataFrame with the same shape as group but filled with None\n",
    "        none_df = pd.DataFrame(None, index=group.index, columns=group.columns)\n",
    "        \n",
    "        # Combine combined_df and group, preserving None values\n",
    "        combined_df = pd.concat([combined_df, none_df], axis=1).combine_first(group)\n",
    "\n",
    "    combined_df = combined_df.sort_values(by=['FAIL_TYPE', param.die_y_name], ascending=[True, False])\n",
    "    combined_df = combined_df.reset_index()\n",
    "\n",
    "    # Rename columns\n",
    "    combined_df = combined_df.rename(columns={param.die_y_name: 'Fail_%',})\n",
    "    return combined_df\n",
    "\n",
    "def add_x_col_in_df(df:pd.DataFrame)->pd.DataFrame:\n",
    "    # Copy the first row\n",
    "    first_row = df.iloc[0]\n",
    "\n",
    "    # Find the indices where FAIL_TYPE changes\n",
    "    change_indices = df[df['FAIL_TYPE'].ne(df['FAIL_TYPE'].shift())].index\n",
    "\n",
    "    # Create a list to store the new DataFrame rows\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate over the DataFrame and insert the copied row at the change points\n",
    "    for i in range(len(df)):\n",
    "        if i in change_indices and i != 0:\n",
    "            new_rows.append(first_row)\n",
    "        new_rows.append(df.iloc[i])\n",
    "\n",
    "    # Create a new DataFrame from the new rows\n",
    "    new_df = pd.DataFrame(new_rows).reset_index(drop=True)\n",
    "    remove_index_0= new_df.drop(index=0).reset_index(drop=True)\n",
    "    return remove_index_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You dataframe column had ['LOT', 'waferID', 'DieX', 'DieY', 'FailBit', 'SB', 'HTB', 'VTB', 'BL', 'partialBL', 'SBL', 'BLK', 'cross', 'others']\n"
     ]
    }
   ],
   "source": [
    "#fail mode is in a wide table format\n",
    "first_level_df = get_decode_raw_file(file_list)\n",
    "first_level_df.to_csv('first_level_df.csv', index=False)\n",
    "\n",
    "second_level_data = group_data(first_level_df, param.fail_type_map)\n",
    "second_level_data.to_csv('second_level_data.csv', index=False)\n",
    "\n",
    "long_table_raw = pivot_wide_table_to_long(second_level_data, param.fail_type_map)\n",
    "long_table = backfill_dummy(long_table_raw, param.fail_type_map)\n",
    "long_table.to_csv('long_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_check_1st_raw = gen_1st_level_to_result_checking(long_table)\n",
    "result_check_1st_raw.to_csv('result_check_1st_raw.csv', index=False)\n",
    "\n",
    "combined_df = gen_pivot_to_wide_form_by_wafer(result_check_1st_raw)\n",
    "combined_df.to_csv('grouped.csv', index=False)\n",
    "\n",
    "viewing_last_df = pd.read_csv('grouped.csv')\n",
    "fin_df = add_x_col_in_df(viewing_last_df)\n",
    "fin_df.to_csv('fin_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the range for DieX and DieY\n",
    "die_range = range(-10, 11)  # From -10 to 10 including 0\n",
    "\n",
    "# Create a list of dictionaries with the mock data\n",
    "data = []\n",
    "for die_x in die_range:\n",
    "    for die_y in die_range:\n",
    "        data.append({\n",
    "            'LOT': 'TKB',\n",
    "            'waferID': 'TKB',\n",
    "            'DieX': die_x,\n",
    "            'DieY': die_y,\n",
    "            'FailBit': 1,\n",
    "            'SB': 30000,\n",
    "            'HTB': 5000,\n",
    "            'VTB': 30 if die_x != 3 else 300,  # Example variation\n",
    "            'BL': 30,\n",
    "            'partialBL': np.random.choice([0, 20]),  # Randomly choose between 0 and 20\n",
    "            'SBL': np.random.choice([1, 23]),       # Randomly choose between 1 and 23\n",
    "            'BLK': np.random.choice([4, 32]),       # Randomly choose between 4 and 32\n",
    "            'cross': np.random.choice([2, 12]),     # Randomly choose between 2 and 12\n",
    "            'others': 32\n",
    "        })\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.to_csv('mock3.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
